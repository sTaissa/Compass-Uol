<h1 align="center"> Desafio </h1>

<p align="center">
 <a href="#sobre">Sobre</a> ‚Ä¢
 <a href="#parte1">Parte 1</a> ‚Ä¢
 <a href="#parte2">Parte 2</a> ‚Ä¢
 <a href="#parte3">Parte 3</a> ‚Ä¢
 <a href="#parte4">Parte 4</a>
</p>

<br> 

<a id="sobre"></a>
## üìé‚ÄäSobre

Ao final do programa de bolsas precisamos entregar um desafio: ingerir dados em batch sobre filmes e s√©ries para um data lake na AWS, realizar uma an√°lise e disponibilizar esses dados para visualiza√ß√£o, extraindo valor dos dados coletados.

### Tema
Cada squad recebeu um g√™nero de filmes/s√©ries para trabalhar em cima, meus g√™neros s√£o anima√ß√£o e/ou com√©dia.

Com base nisso o tema da minha an√°lise e desafio final √©: **Os melhores e piores filmes da Disney lan√ßados no s√©culo 21**

<a id="parte1"></a>
## üì§‚ÄäParte 1 - ETL

> Ingest√£o Batch: a ingest√£o dos arquivos CSV em Bucket Amazon S3 RAW Zone. Nesta etapa do desafio deve ser constru√≠do um c√≥digo Python que ser√° executado dentro de um container Docker para carregar os dados locais dos arquivos para a nuvem. Nesse caso utilizaremos, principalmente, as lib [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) como parte do processo de ingest√£o via batch para gera√ß√£o de arquivo (CSV).
>
>![parte 1](/desafio/imagens-readme/parte1.png)

<br>

>1.Implementar c√≥digo Python:
>- ler os 2 arquivos ([filmes e series](/desafio/parte1-etl/Filmes%2Be%2BSeries.zip)) no formato CSV inteiros, ou seja, sem filtrar os dados
>- utilizar a lib boto3 para carregar os dados para a AWS
>- acessar a AWS e gravar no S3, no bucket definido com RAW Zone
>
> No momento da grava√ß√£o dos dados deve-se considerar o padr√£o: <nome do bucket>\<camada de armazenamento>\<origem do dado>\<formato do dado>\<especifica√ß√£o do dado>\<data de processamento separada por ano\mes\dia>\<arquivo>
> 
> Por exemplo:
>-  S3:\\data-lake-do-fulano\Raw\Local\CSV\Movies\2022\05\02\movies.csv
>- S3:\\data-lake-do-fulano\Raw\Local\CSV\Series\2022\05\02\series.csv

Acesse o c√≥digo que carrega os arquivos csv para o S3: [meu c√≥digo](/desafio/parte1-etl/etl.py)


>2. Criar container Docker com um volume para armazenar os arquivos CSV e executar processo Python implementado
>3. Executar localmente o container docker para realizar a carga dos dados ao S3

Dockerfile para criar a imagem do container: [dockerfile](/desafio/parte1-etl/Dockerfile)

Comandos docker para criar a imagem e o container com um volume para persistir os arquivos csv:

![comandos dockes](/desafio/imagens-readme/parte1-comandos.PNG)

Eu utilizei um arquivo .env com as chaves de acesso aws configuradas como vari√°veis de ambiente e passei esse arquivo para o docker com a tag "--env-file" para que o c√≥digo pudesse usar esses valores como vari√°veis de ambiente. Dessa forma as credenciais n√£o ficam diretamente no c√≥digo e √© pr√°tico pra atualiz√°-las.

Bucket com os arquivos upados:

![bucket](/desafio/imagens-readme/parte1-bucket.PNG)

<a id="parte2"></a>
## üì§‚ÄäParte 2 - Ingest√£o de dados do Twitter e/ou TMBD

>Neste etapa do desafio, nos iremos capturar tweets em tempo real com Python por meio da lib tweepy e/ou dados existentes na API do TMDB via AWS Lambda. Os dados coletados devem ser persistidos em Amazon S3, camada de RAW Zone, mantendo o formato da origem (JSON) e, se poss√≠vel, agrupando-os em arquivos com, no m√°ximo, 100 tweets cada
>
>O objetivo desta etapa √© enriquecer os dados dos Filmes e Series carregados na Etapa 1 com dados externos do Twitter e/ou do TMDB e/ou de outra API a sua escolha.
>
>Importante:
>
>- Os arquivos CSVs carregados na Etapa 1 n√£o devem ser modificados.
>- Os novos dados devem ser complementares aos dados do CSV. Tem que existir informa√ß√µes novas sobre os dados do CSV.
>- N√£o √© necess√°rio realizar tratamento dos dados externos, o m√°ximo que pode ser feito √© o agrupamento de dados.
>- Cuidado para os arquivos JSON gerados n√£o serem maior do que 10 MB.
>- N√£o agrupe JSON com estruturas diferentes.
>- Se voc√™ escolher por fazer o desafio por todos atores ou s√©ries ou filmes de uma ou mais categorias, utilize o CSV carregado na Etapa 1 como fonte de entrada para localiza√ß√£o dos IDs do IMDB para depois realizar a pesquisa no TMDB.
>- Se voc√™ escolher fazer sobre um filme ou uma trilogia espec√≠fica, considere utilizar pelo menos 4 m√©todos de API diferentes para possibilitar uma an√°lise de dados qualificada.
>- Considere desenvolver seu c√≥digo localmente primeiro e com poucos dados para depois leva-lo para a AWS Lambda e aumentar a pesquisa de dados. APIs normalmente limitam requisi√ß√µes. Evite realizar muitas requisi√ß√µes em fase de desenvolvimento ou teste para evitarmos qualquer bloqueio na conta de voc√™s
>
>![imagem parte 2](/desafio/imagens-readme/parte2.png)

>Em sua conta AWS, no servi√ßo AWS Lambda, realize as seguintes atividades:
>1.  Criar nova camada (layer) no AWS Lambda para as libs necess√°rias √† ingest√£o de dados (por exemplo,  tweepy, se voc√™ utilizar o Tweeter)

Como n√£o usei a API do Twitter, criei uma camada para poder usar Pandas no c√≥digo AWS Lambda

>2. Implementar o c√≥digo Python em AWS Lambda para consumo de dados do Twitter/TMDB:
>   - Se est√° utilizando Twitter, buscar os tweets de interesse para a an√°lise (neste ponto voc√™ j√° deve ter definido qual an√°lise planeja realizar com os dados) e agrupar os tweets em arquivo JSON com, no m√°ximo, 100 registros cada
>   - Se est√° utilizando TMDB,  buscar pela API os dados que complementem a an√°lise
>   - Utilizar a lib boto3 para gravar os dados no AWS S3
>
>No momento da grava√ß√£o dos dados deve-se considerar o padr√£o de path: <nome do bucket>\<camada de armazenamento>\<origem do dado>\<formato do dado>\<especifica√ß√£o do dado>\<data de processamento separada por ano\mes\dia>\<arquivo>
>
> S√£o exemplos de caminhos de arquivos v√°lidos:
>- S3:\\data-lake-do-fulano\Raw\Twitter\JSON\2022\05\02\prt-uty-nfd.json
>- S3:\\data-lake-do-fulano\Raw\Twitter\JSON\2022\05\02\idf-uet-wqt.json

Acesse meu c√≥digo que consome os dados da API TMDB para os filmes de anima√ß√£o do s√©culo 21, como um complemento das informa√ß√µes do csv: [c√≥digo](/desafio/parte2-ingestao/tmdb-ingestao.py)

Dados j√° upados no S3 pelo c√≥digo:

![bucket](/desafio/imagens-readme/parte2-bucket.PNG)
![bucket2](/desafio/imagens-readme/parte2-bucket1.PNG)

>3. Caso esteja utilizando o Twitter, execute a fun√ß√£o Lambda periodicamente para alimentar seu conjunto de dados no S3.
>
>Informa√ß√£o adicional:
>Podemos utilizar os servi√ßos  CloudWatch Event ou Amazon EventBridge para agendar extra√ß√µes peri√≥dicas de dados no Twitter de forma autom√°tica.

### Observa√ß√µes:
Para ser poss√≠vel acessar o servi√ßo S3 dentro da fun√ß√£o Lambda, foi preciso criar uma pol√≠tica de permiss√£o para a fun√ß√£o no AWS IAM, com o seguinte c√≥digo:
``` JSON
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "s3:PutObject",
                "s3:GetObject"
            ],
            "Resource": "arn:aws:s3:::datalake-taissa/*"
        }
    ]
}
```
Al√©m disso, foi preciso configurar uma pol√≠tica do bucket para permitir esse acesso tamb√©m:
``` Json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "PublicReadGetObject",
            "Effect": "Allow",
            "Principal": "*",
            "Action": [
                "s3:GetObject",
                "s3:PutObject"
            ],
            "Resource": "arn:aws:s3:::datalake-taissa/*"
        }
    ]
}
```

<a id="parte3"></a>
## üì§‚ÄäParte 3 - Transforma√ß√£o e filtragem dos dados

![parte 3 do desafio](/desafio/imagens-readme/parte3.png)

### Processamento -  Camada Trusted
>A camada Trusted de um data lake corresponde √†quela em que os dados encontram-se limpos e s√£o confi√°veis. √â resultado da integra√ß√£o das diversas fontes de origem, que encontram-se na camada anterior, que chamamos de Raw.
>
>Aqui faremos uso do Apache Spark no processo, integrando dados existentes na camada Raw Zone. O objetivo √© gerar uma vis√£o padronizada dos dados, persistida no S3,  compreendendo a Trusted Zone do data lake.  Nossos jobs Spark ser√£o criados por meio do AWS Glue.
>
>Todos os dados ser√£o persistidos na Trusted no formato PARQUET, particionados por data de cria√ß√£o do tweet  ou data de coleta do TMDB (dt=<ano\m√™s\dia> exemplo: dt=2018\03\31). A exce√ß√£o fica para os dados oriundos do processamento batch (CSV), que n√£o precisam ser particionados.
>
>Iremos separar o processamento em dois jobs: o primeiro, para carga hist√≥rica, ser√° respons√°vel pelo processamento dos arquivos CSV  e o segundo, para carga de dados do Twitter/TMDB. Lembre-se que suas origens ser√£o os dados existentes na RAW Zone.

C√≥digo com o job Spark do Glue para processar os dados do arquivo de [movies.csv](/desafio/parte1-etl/dados/Filmes%2Be%2BSeries.zip) e salv√°-los na trusted: [trusted movies](/desafio/parte3/processamento-trusted/trusted-csv.py)

C√≥digo com o job Spark do Glue para processar os dados dos arquivos json gerados a partir da API TMDB na etapa anterior: [trusted TMDB](/desafio/parte3/processamento-trusted/trusted-json.py)

C√≥digo com o job Spark do Glue para processar os dados do arquivo [oscar.csv](/desafio/parte1-etl/dados/oscar.csv) criado por mim para complementar a an√°lise: [trusted oscar](/desafio/parte3/processamento-trusted/trusted-oscar.py)

Arquivos parquet salvos na camada trusted no meu bucket: 
![bucket trusted](/desafio/imagens-readme/parte3-trusted.png)

<br>

### Modelagem de dados da refined

>A camada Refined corresponde √† camada de um data lake em que os dados est√£o prontos para an√°lise e extra√ß√£o de insights. Sua origem corresponde aos dados da camada anterior, a Trusted.
>
>Devemos pensar em estruturar os dados seguindo os princ√≠pios de modelagem multidimensional, a fim de permitir consultas sobre diferentes perspectivas.
>
>Crie a modelagem dimensional com os dados que ir√° usar para a an√°lise final

Modelos de dados que ser√° usado para criar a camada refined:
![modelagem refined](/desafio/imagens-readme/parte3-modelagem.PNG)

<br>

### Processamento - Camada Refined
>Na atividade anterior, voc√™ definiu seu modelo de dados da camada Trusted. Agora √© tempo de processar os dados da camada Trusted, armazena-os na Refined, de acordo com seu modelo.
>
>Aplicaremos novamente o Apache Spark no processo, utilizando jobs cuja origem sejam dados da camada Trusted Zone e e o destino, a camada Refined Zone.  Aqui, novamente, todos os dados ser√£o persistidos no formato PARQUET, particionados, se necess√°rio,  de acordo com as necessidades definidas para a camada de visualiza√ß√£o.

C√≥digo com o job Saprk do Glue para processar os dados que ir√£o para a camada refined e criar as tabelas no Glue Catalog: [c√≥digo refined](/desafio/parte3/processamento-refined/refined.py)

Camada refined criada no bucket com os arquivos parquet:
![bucket refined](/desafio/imagens-readme/parte3-refined.PNG)

Tabelas criadas no Glue Catalog:
![glue catalog](/desafio/imagens-readme/parte3-glue-catalog.PNG)